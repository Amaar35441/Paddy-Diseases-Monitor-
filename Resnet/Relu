from tensorflow.keras import backend as K
K.clear_session()



!pip install rarfile

import os
import gdown
import rarfile
from PIL import Image

# Extracted file ID from Google Drive link
file_id = '1fOFS9igMw1Vo8n64IZBBJHp2NXFi_tT3'
url = f'https://drive.google.com/uc?id={file_id}'  # Correct download link format
output_rar = '/content/preprocess_data.rar'

# Download the RAR file
gdown.download(url, output_rar, quiet=False)

# Verify and extract the RAR file
if rarfile.is_rarfile(output_rar):
    with rarfile.RarFile(output_rar, 'r') as rar_ref:
        rar_ref.extractall('/content')
    print("Extraction successful.")
else:
    print("The downloaded file is not a valid RAR file.")





# Path to the directory where the RAR file was extracted
data_dir = '/content/preprocessed_data'



from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Add, AveragePooling2D, Flatten, Dense, LeakyReLU
from tensorflow.keras.optimizers import Adam

# Define the Residual Block with Leaky ReLU
def residual_block(x, filters, downsample=False):
    shortcut = x
    stride = 1
    if downsample:
        shortcut = Conv2D(filters, (1, 1), strides=2, padding='same')(shortcut)
        shortcut = BatchNormalization()(shortcut)
        stride = 2

    x = Conv2D(filters, (3, 3), strides=stride, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)

    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    x = Add()([x, shortcut])
    x = LeakyReLU(alpha=0.01)(x)
    return x

# Define the ResNet model with Leaky ReLU
def build_resnet(input_shape=(224, 224, 3), num_classes=5):
    inputs = Input(shape=input_shape)

    # Initial convolutional layer
    x = Conv2D(64, (7, 7), strides=2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Stack of residual blocks with consistent filter sizes
    x = residual_block(x, 64)
    x = residual_block(x, 128, downsample=True)
    x = residual_block(x, 128)
    x = residual_block(x, 256, downsample=True)
    x = residual_block(x, 256)
    x = residual_block(x, 512, downsample=True)
    x = residual_block(x, 512)

    # Global average pooling and output layer
    x = AveragePooling2D((7, 7))(x)
    x = Flatten()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Instantiate and summarize the model with Leaky ReLU
model = build_resnet(num_classes=5)
model.summary()






import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Directory for the dataset
dataset = data_dir # Adjust path if necessary

# Define classes
classes = ['bacterial_leaf_blight', 'brown_spot', 'healthy', 'leaf_blast', 'Rice Hispa']

# ImageDataGenerators for training, validation, and testing
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    os.path.join(data_dir, 'Train'),
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    shuffle=True
)

val_generator = val_datagen.flow_from_directory(
    os.path.join(data_dir, 'Validation'),
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    shuffle=False
)

test_generator = test_datagen.flow_from_directory(
    os.path.join(data_dir, 'Test'),
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    shuffle=False
)

# Train the model
history = model.fit(train_generator, epochs=10, validation_data=val_generator)

# Evaluate the model on test data
def evaluate_model(test_generator, model, classes, title="Performance"):
    y_test_pred = model.predict(test_generator)
    y_test_pred = np.argmax(y_test_pred, axis=1)

    precision = precision_score(test_generator.labels, y_test_pred, average='weighted')
    recall = recall_score(test_generator.labels, y_test_pred, average='weighted')
    f1 = f1_score(test_generator.labels, y_test_pred, average='weighted')
    accuracy = accuracy_score(test_generator.labels, y_test_pred)

    print(f"\n{title}")
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("Accuracy:", accuracy)

    # Confusion matrix
    confusion = confusion_matrix(test_generator.labels, y_test_pred)
    confusion_df = pd.DataFrame(confusion, index=classes, columns=classes)
    print("Confusion Matrix:\n", confusion_df)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {title}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Evaluate model on the test set
evaluate_model(test_generator, model, classes, title="Performance on Test Data")

